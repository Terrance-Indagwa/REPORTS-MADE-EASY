[["index.html", "REPORTS MADE EASY Prerequisites 0.1 Packages.", " REPORTS MADE EASY DIR-TERRANCE1 2021-02-09 Prerequisites 0.1 Packages. Install the following packages to follow the book easily. #install.packages(&quot;bookdown&quot;) #to generate the book. ## or the development version ## devtools::install_github(&quot;rstudio/bookdown&quot;) library(bookdown) # #install.packages(&quot;tidyverse&quot;)# a collection of r packages that makes working with data easier. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.6 ✓ dplyr 1.0.4 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # #install.packages(&quot;rticles&quot;)# journal like template library(rticles) # #install.packages() # library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose library(RColorBrewer) 0.1.1 Bibliography automatically create a bib database for R packages knitr::write_bib(c( .packages(), &#39;bookdown&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39;, &#39;tidyverse&#39; ), &#39;packages.bib&#39;) FOUNDER:RWILLS STATISTICAL COMPANY,EMAIL:consultancyrwillsstats@gmail.com↩︎ "],["intro.html", "Chapter 1 Introduction 1.1 THE CHALLLENGER. 1.2 DATA", " Chapter 1 Introduction 1.1 THE CHALLLENGER. If you have ever been in a data analytics class I bet you have ever heard of Edward Tufte. On his criticisms on the engineers’ failures to make sense of the concerns the had on lunching The Challenger on January 28, 1986. Being Right isn’t enough, you have to be convincing. ~ Edward Tufte This philosophy formed the basis of his arguments in his book Visual Explanations and Quantities, Evidence and Narrative An essential analytic task in making decisions based on evidence is to understand how things work—mechanism, trade-offs, process and dynamics, cause and effect. That is, intervention-thinking and policy-thinking demand causality-thinking Making decisions based on evidence requires the appropriate display of that evidence. Good displays of data help to reveal knowledge relevant to understanding mechanism, process and dynamics, cause and effect. That is, displays of statistical data should directly serve the analytic task at hand. Hence we are going set our reports right even before we begin in whatever we are supposed to report on. 1.2 DATA Lets take an hypothetical data where in Africa the distance students take to walk to get access to basic education affect their perfomance. set.seed(123) n &lt;- 30 df &lt;- tibble( score=sample(30:99, size = n, replace = T), dist=sample(1:8, size=n, replace = T) ) head(df)#Checking the first six observations ## # A tibble: 6 x 2 ## score dist ## &lt;int&gt; &lt;int&gt; ## 1 60 8 ## 2 80 6 ## 3 43 1 ## 4 96 2 ## 5 71 5 ## 6 79 5 ### Any time you get data into your hands always check its health status ie, the first thing you get in a hospital is checking weight, height, BMI, blood pressure etc. ### ### Height &amp; Weight nrow(df)#number of rows ## [1] 30 ncol(df)#number of columns ## [1] 2 ### Pressure str(df)#structure ## tibble [30 × 2] (S3: tbl_df/tbl/data.frame) ## $ score: int [1:30] 60 80 43 96 71 79 72 43 54 98 ... ## $ dist : int [1:30] 8 6 1 2 5 5 8 4 7 5 ... ### BMI dim(df) ## [1] 30 2 colnames(df) ## [1] &quot;score&quot; &quot;dist&quot; ### "],["eda.html", "Chapter 2 EXPLANATORY DATA ANALYSIS 2.1 DATA WRANGLING 2.2 Visualization", " Chapter 2 EXPLANATORY DATA ANALYSIS Here we form the foundation of our report. We listen to what the data has for us.This should be the entry step in a data project, where we start by knowing the correct data types and exploring distributions in numerical and categorical variables. Always getting your data into the right format will affect how you work with them hence inferences that they generate. At this point we perform dataset ‘anatomy’ namely; *Getting metrics like total rows, columns, data types, zeros, and missing values *How each of the previous items impacts on different analysis *How to quickly filter and select variables. *Univariate analysis in categorical variable: +Frequency, percentage, cumulative value, and colorful plots *Univariate analysis with numerical variables: +Percentile, dispersion, standard deviation, mean, top and bottom values +Plotting distributions Based on our 1.2 we can develop the following questions. *Can we divide the perfomance into fail and passed? *What distance community does one above categories fall into? 2.1 DATA WRANGLING df$performance &lt;- fifelse(df$score&gt;=40, yes = &quot;passed&quot;, no=&quot;failed&quot;) #answering the first question. df$dist_comm &lt;- fifelse(df$dist&lt;=1, yes = &quot;near&quot;, fifelse(df$dist&gt;1 &amp; df$dist&lt;=4, yes=&quot;average&quot;, no=&quot;far&quot;) )#answering the second question. df$dist &lt;- as.factor(df$dist) df$dist_comm &lt;- as.factor(df$dist_comm) df$performance &lt;- as.factor(df$performance) 2.2 Visualization df %&gt;% ggplot(aes(dist, score))+ geom_boxplot()+ labs(title = &quot;BOXPLOT OF STUDENTS&#39;\\nSCORE BASED ON DISTANCE.&quot;) Figure 2.1: boxplot of score based pn distance students walk Its results above is astonishing, for see students in coming from distance 3-4 perform even better than those who are close to school. df %&gt;% ggplot(aes(y=score, fill=performance))+ geom_bar(position = &quot;stack&quot;)+ facet_wrap(~performance)+ labs(title = &quot;Bargraph OF STUDENTS&#39;\\nSCORE.&quot;) Figure 2.2: Bargraphs of students score perfomance based on different factors. df %&gt;% ggplot(aes(y=score, fill=performance))+ geom_bar(position = &quot;identity&quot;)+ facet_wrap(~dist_comm)+ labs(title = &quot;Bargraph OF STUDENTS&#39;\\nSCORE. IN DIFFERENT DISTANCE COMMUNITIES&quot;) Figure 2.3: Bargraphs of students score perfomance based on different factors. "],["mods.html", "Chapter 3 MODELS/METHODS 3.1 USES OF STATISTICAL MODELS{#statMods}2.", " Chapter 3 MODELS/METHODS Now that we have seen some interesting patterns, lets try to get the finnier details of them. Modeling is important for they reveal subtle stories in a data. All models are wrong. Some models are useful. ~Author(Statistical Modelling) Art is a lie that tells the truth! ~Pablo Picasso There are as many named lists of models as sand, yet they all have one thing in common; A model is a representative of a specific purpose. Hence the appropriate form of the model depends on the task at hand and the expertise of the scientist. 3.1 USES OF STATISTICAL MODELS{#statMods}2. *Description models *Classification or prediction models. *Anticipating the consequences of interventions models. Its difficult to use observation to inform concrete knowledge because r/ships are complicated and involve multiple factors, do you still recall the mistakes made by the challengers’ engineers?1.1. Mathematicians(“Statisticians”) do not study objects, butbthe relations among objetcs. ~Georges Braque Lets consider our case data 2.1, we want to build a simple model that predicts a students performance based on the distance community(Sometimes variables will have collinearity hence be keen.) \\[\\begin{equation} Y=\\beta_0+\\beta X +e (\\#eq: reg) \\end{equation}\\] mod &lt;- lm(formula =score~dist_comm, data = df) anova(mod) ## Analysis of Variance Table ## ## Response: score ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dist_comm 2 2240.1 1120.03 3.5759 0.04191 * ## Residuals 27 8456.9 313.22 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod) ## ## Call: ## lm(formula = score ~ dist_comm, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.471 -13.153 -1.971 10.279 42.625 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.375 6.257 8.530 3.82e-09 *** ## dist_commfar 16.096 7.588 2.121 0.0432 * ## dist_commnear -3.175 10.089 -0.315 0.7554 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.7 on 27 degrees of freedom ## Multiple R-squared: 0.2094, Adjusted R-squared: 0.1508 ## F-statistic: 3.576 on 2 and 27 DF, p-value: 0.04191 There are several classification of models eg. Mathematical and Statistical models, how you define a model changes its classification hence affecting its application↩︎ "],["disc.html", "Chapter 4 DISCUSSION", " Chapter 4 DISCUSSION From the anova table we can see that slightly significant with significance level at \\(0.01\\). Furthermore we can conclude from the summary table that distance average is the most significant in the three categories of dist_comm, which we can interpret for every one unit change in dist_commaverage, dist_commfar increases by $0.0432$. Another good habit you should always check is measure of goodness of fit of your model here we look at how good is our model working. Checking the adjusted R-Squared(\\(.1508\\)) we see the model poorly performed this is justified with the high RSE(\\(17.7\\)). Always remember even is our model poorly performs it will always tell us something as per this definition of statistics, “Statistics is the explanation of patterns in the context of what remains unknown” "],["recommendation.html", "Chapter 5 RECOMMENDATION", " Chapter 5 RECOMMENDATION Looking at the eda3 and the discussion4 we can conclude our analytics work need more data or there maybe some information missing, to correctly predict a students score. Hence, the data department should review the data for any further analytics. "],["remarks.html", "A REMARKS", " A REMARKS Awesome unlike the engineers you will never have to face the Tuftes’ criticisms. Ofcourse this just an introduction tutorial I have tried to capture the essentials as much as I can. "],["references.html", "References", " References Xie Y. (2020). bookdown:Authoring Books and Techincal Documents using R Markdown R package version 1.31. R Core Team(2020). R: A LAnguage and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. "]]
